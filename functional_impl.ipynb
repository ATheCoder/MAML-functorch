{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml_functorch.model import VinyalsConv\n",
    "from maml_functorch.trainer import HyperParameters\n",
    "from maml_functorch.dataset import load_data, DataConfig\n",
    "from maml_functorch.utils import get_accuracy_from_logits\n",
    "from maml_functorch.testing_utils import generate_random_batch\n",
    "\n",
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = VinyalsConv(5, embedding_feats=288, track_running_stats=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_set, support_labels, query_set, query_labels = generate_random_batch(5, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "\n",
    "fmodel, params, buffers = make_functional_with_buffers(model, disable_autograd_tracking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel(params, buffers, support_set[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_meta_loss(params, buffers, support_sample, support_labels):\n",
    "    logits = fmodel(params, buffers, support_sample)\n",
    "    support_loss = F.cross_entropy(logits, support_labels)\n",
    "\n",
    "    return support_loss\n",
    "\n",
    "compute_meta_grads = grad(compute_meta_loss)\n",
    "\n",
    "def calculate_next_params(params, buffers, support_sample, support_labels):\n",
    "    grads = compute_meta_grads(params, buffers, support_sample, support_labels)\n",
    "    \n",
    "    new_params = [p - 0.01 * g for p, g in zip(params, grads)]\n",
    "    \n",
    "    return new_params\n",
    "\n",
    "def compute_logits(params, buffers, support_sample, support_labels, query_sample):\n",
    "    last_params = params\n",
    "    for i in range(5):\n",
    "        last_params = calculate_next_params(last_params, buffers, support_sample, support_labels)\n",
    "    \n",
    "    return fmodel(last_params, buffers, query_sample)\n",
    "\n",
    "compute_logits(params, buffers, support_set[0], support_labels[0], query_set[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_compute_logits = vmap(compute_logits, in_dims=(None, None, 0, 0, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50, 3, 84, 84])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query Shape: torch.Size([4, 10, 3, 84, 84])\n",
    "print(query_set.shape)\n",
    "batch_compute_logits(params, buffers, support_set, support_labels, query_set).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fo_grads(params, buffers, support_sample, support_labels, query_sample, query_labels):\n",
    "    last_params = params\n",
    "    for i in range(5):\n",
    "        grads = compute_meta_grads(params, buffers, support_sample, support_labels)\n",
    "    \n",
    "        last_params = [p - 0.01 * g for p, g in zip(params, grads)]\n",
    "    \n",
    "    \n",
    "    return compute_meta_grads(last_params, buffers, query_sample, query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(params, buffers, support_sample, support_labels, query_sample, query_labels):\n",
    "    last_params = params\n",
    "    for i in range(5):\n",
    "        grads = compute_meta_grads(params, buffers, support_sample, support_labels)\n",
    "    \n",
    "        last_params = [p - 0.01 * g for p, g in zip(params, grads)]\n",
    "    \n",
    "    \n",
    "    return compute_meta_loss(last_params, buffers, query_sample, query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_so_grads = grad(compute_loss)\n",
    "batch_so_grads = vmap(compute_so_grads, in_dims=(None, None, 0, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_grads_maml = grad(compute_loss_stateless_model)\n",
    "\n",
    "# example_support, supprot_labels, example_query, query_labels = generate_random_task()\n",
    "\n",
    "\n",
    "# compute_fo_grads(params, buffers, example_support, supprot_labels, example_query, query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_fo_grads = vmap(compute_fo_grads, in_dims=(None, None, 0, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support_samples, support_labels, query_samples, query_labels = generate_random_batch(4)\n",
    "\n",
    "\n",
    "# print(support_samples.shape)\n",
    "# print(support_labels.shape)\n",
    "\n",
    "# print(query_samples.shape)\n",
    "# print(query_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = batch_fo_grads(params, buffers, support_samples, support_labels, query_samples, query_labels)\n",
    "\n",
    "# print(support_samples.shape)\n",
    "# print(support_labels.shape)\n",
    "\n",
    "# print(len(params))\n",
    "# print(len(grads))\n",
    "\n",
    "# for p, g in zip(params, grads):\n",
    "#     print(p.shape)\n",
    "#     print(g.sum(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_updater_factory(lr: float):\n",
    "    def update_params(params, grads):\n",
    "        with torch.no_grad():\n",
    "            return [p.add_( -lr * g.sum(dim=0) / 4) for p, g in zip(params, grads)]\n",
    "    \n",
    "    return update_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(logits, labels):\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "batch_cross_entropy = vmap(cross_entropy, in_dims=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mathecoder\u001b[0m (\u001b[33mdest\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f328b29bea04b819a97c66f4d775193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666813326666367, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/athecoder/MAML-functorch/wandb/run-20230210_142429-3vgmi42p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dest/MAML-functorch/runs/3vgmi42p\" target=\"_blank\">honest-sponge-1</a></strong> to <a href=\"https://wandb.ai/dest/MAML-functorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 171/20000 [00:09<18:24, 17.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 52\u001b[0m\n\u001b[1;32m     47\u001b[0m                 valid_acc \u001b[39m=\u001b[39m get_accuracy_from_logits(logits, query_labels)\n\u001b[1;32m     49\u001b[0m                 wandb\u001b[39m.\u001b[39mlog({\u001b[39m'\u001b[39m\u001b[39mvalid_task_acc\u001b[39m\u001b[39m'\u001b[39m: valid_acc, \u001b[39m'\u001b[39m\u001b[39mvalid_task_loss\u001b[39m\u001b[39m'\u001b[39m: valid_loss\u001b[39m.\u001b[39msum()})\n\u001b[0;32m---> 52\u001b[0m train(\n\u001b[1;32m     53\u001b[0m     HyperParameters(\n\u001b[1;32m     54\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m20_000\u001b[39;49m,\n\u001b[1;32m     55\u001b[0m         alpha\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m     56\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m     57\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     58\u001b[0m         num_meta_learn_loop\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     59\u001b[0m         second_order\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     60\u001b[0m         shots\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     61\u001b[0m         ways\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     62\u001b[0m         embedding_feats\u001b[39m=\u001b[39;49m\u001b[39m288\u001b[39;49m,\n\u001b[1;32m     63\u001b[0m         query_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m )\n",
      "Cell \u001b[0;32mIn[38], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     18\u001b[0m query_labels \u001b[39m=\u001b[39m train_batch[\u001b[39m'\u001b[39m\u001b[39mquery_labels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcuda(non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39msecond_order \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     grads \u001b[39m=\u001b[39m batch_so_grads(params, buffers, support_samples, support_labels, query_samples, query_labels)\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     grads \u001b[39m=\u001b[39m batch_fo_grads(params, buffers, support_samples, support_labels, query_samples, query_labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/vmap.py:362\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[1;32m    361\u001b[0m batch_size, flat_in_dims, flat_args, args_spec \u001b[39m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[0;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    363\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    364\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/vmap.py:489\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 489\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    491\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/eager_transforms.py:1241\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1241\u001b[0m     results \u001b[39m=\u001b[39m grad_and_value(func, argnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1242\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m   1243\u001b[0m         grad, (_, aux) \u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/eager_transforms.py:1111\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m diff_args \u001b[39m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1109\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39mlevel), diff_args)\n\u001b[0;32m-> 1111\u001b[0m output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m   1113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(output, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(output) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(params, buffers, support_sample, support_labels, query_sample, query_labels)\u001b[0m\n\u001b[1;32m      2\u001b[0m last_params \u001b[39m=\u001b[39m params\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     grads \u001b[39m=\u001b[39m compute_meta_grads(params, buffers, support_sample, support_labels)\n\u001b[1;32m      6\u001b[0m     last_params \u001b[39m=\u001b[39m [p \u001b[39m-\u001b[39m \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m g \u001b[39mfor\u001b[39;00m p, g \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params, grads)]\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m compute_meta_loss(last_params, buffers, query_sample, query_labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/eager_transforms.py:1241\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1241\u001b[0m     results \u001b[39m=\u001b[39m grad_and_value(func, argnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1242\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m   1243\u001b[0m         grad, (_, aux) \u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/eager_transforms.py:1133\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m flat_outputs \u001b[39m=\u001b[39m _as_tuple(output)\n\u001b[0;32m-> 1133\u001b[0m flat_grad_input \u001b[39m=\u001b[39m _autograd_grad(flat_outputs, flat_diff_args, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1134\u001b[0m grad_input \u001b[39m=\u001b[39m tree_unflatten(flat_grad_input, spec)\n\u001b[1;32m   1136\u001b[0m grad_input \u001b[39m=\u001b[39m _undo_create_differentiable(grad_input, level)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/functorch/_src/eager_transforms.py:113\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(diff_outputs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs)\n\u001b[0;32m--> 113\u001b[0m grad_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(diff_outputs, inputs, grad_outputs,\n\u001b[1;32m    114\u001b[0m                                   retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    115\u001b[0m                                   create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    116\u001b[0m                                   allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    117\u001b[0m grad_inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mif\u001b[39;00m gi \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gi\n\u001b[1;32m    118\u001b[0m                     \u001b[39mfor\u001b[39;00m gi, inp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(grad_inputs, inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m grad_inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_1.3/lib/python3.10/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train(args: HyperParameters):\n",
    "    model = VinyalsConv(args.ways, embedding_feats=288, track_running_stats=False).to(device)\n",
    "    fmodel, params, buffers = make_functional_with_buffers(model, disable_autograd_tracking=True)\n",
    "    \n",
    "    train_iter, test_iter = load_data(DataConfig(True, args.ways, args.shots, args.batch_size, args.query_size))\n",
    "    \n",
    "    update_params = param_updater_factory(args.beta)\n",
    "    \n",
    "    wandb.init(config=asdict(args))\n",
    "    \n",
    "    for e in tqdm(range(args.epochs)):\n",
    "        train_batch = next(train_iter)\n",
    "        \n",
    "        support_samples = train_batch['support_set'].cuda(non_blocking=True)\n",
    "        support_labels = train_batch['support_labels'].cuda(non_blocking=True)\n",
    "        \n",
    "        query_samples = train_batch['query_set'].cuda(non_blocking=True)\n",
    "        query_labels = train_batch['query_labels'].cuda(non_blocking=True)\n",
    "        \n",
    "        if args.second_order == True:\n",
    "            grads = batch_so_grads(params, buffers, support_samples, support_labels, query_samples, query_labels)\n",
    "        else:\n",
    "            grads = batch_fo_grads(params, buffers, support_samples, support_labels, query_samples, query_labels)\n",
    "        \n",
    "        params = update_params(params, grads)\n",
    "        \n",
    "        if e % 200 == 0 and e != 0:\n",
    "            with torch.no_grad():\n",
    "                test_batch = next(test_iter)\n",
    "                \n",
    "                support_samples = test_batch['support_set'].cuda(non_blocking=True)\n",
    "                support_labels = test_batch['support_labels'].cuda(non_blocking=True)\n",
    "                \n",
    "                query_samples = test_batch['query_set'].cuda(non_blocking=True)\n",
    "                query_labels = test_batch['query_labels'].cuda(non_blocking=True)\n",
    "                \n",
    "                logits = batch_compute_logits(\n",
    "                    params,\n",
    "                    buffers,\n",
    "                    support_samples,\n",
    "                    support_labels,\n",
    "                    query_samples,\n",
    "                )\n",
    "                \n",
    "                valid_loss = batch_cross_entropy(logits, query_labels)\n",
    "                \n",
    "                valid_acc = get_accuracy_from_logits(logits, query_labels)\n",
    "                \n",
    "                wandb.log({'valid_task_acc': valid_acc, 'valid_task_loss': valid_loss.sum()})\n",
    "            \n",
    "\n",
    "train(\n",
    "    HyperParameters(\n",
    "        epochs=20_000,\n",
    "        alpha=0.01,\n",
    "        beta=0.01,\n",
    "        batch_size=4,\n",
    "        num_meta_learn_loop=5,\n",
    "        second_order=True,\n",
    "        shots=1,\n",
    "        ways=5,\n",
    "        embedding_feats=288,\n",
    "        query_size=10\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a263086846ea0a3fcddfd46d624ed79a8d94e93ffdab254f7342d5d1c9c001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
